Contrastive losses are a type of loss functions that are typically used in unsupervised or semi-supervised learning. They are particularly used in tasks like representation learning. The key idea behind contrastive losses is to ensure that similar or "positive" pairs of examples are brought closer together in the feature space, while dissimilar or "negative" pairs of examples are pushed apart. 

- **Positive pair** can consist of a pair of images that are considered similar, for example two images of the same object
- **Negative pair** can consist of examples that are considered not similar. 